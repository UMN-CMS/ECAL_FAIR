{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "precise-needle",
   "metadata": {},
   "source": [
    "This is the notebook to run the GRU network. It starts with a preprocessed file with p2_calib being the value to predict, and int_deliv_inv_ub and calib_time being the best values to use. It's important to note that at this point, some of the lumi measurements are taken a bit far from the calibration times. this can be seen by the difference between the lumi-section time and the calib_time values. The best data probably just consistents of the points where these are close.\n",
    "\n",
    "All data is in its original units.\n",
    "\n",
    "Also, I'm running this in the Python 3.7.1 Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "assisted-excellence",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the stuff\n",
    "import pandas as pd #dataframes etc\n",
    "import matplotlib.pyplot as plt #plotting\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from common.utils import TimeSeriesTensor, create_evaluation_df, mape, scale_shrinker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "imported-hamilton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p2</th>\n",
       "      <th>int_deliv_inv_ub</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-05-12 09:01:31</th>\n",
       "      <td>0.000267</td>\n",
       "      <td>1.912590e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-12 10:01:31</th>\n",
       "      <td>0.000199</td>\n",
       "      <td>1.786448e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-12 11:01:31</th>\n",
       "      <td>0.000203</td>\n",
       "      <td>1.664305e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-12 12:01:31</th>\n",
       "      <td>0.000395</td>\n",
       "      <td>3.387666e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-12 13:01:31</th>\n",
       "      <td>0.000471</td>\n",
       "      <td>3.200991e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           p2  int_deliv_inv_ub\n",
       "2018-05-12 09:01:31  0.000267      1.912590e+07\n",
       "2018-05-12 10:01:31  0.000199      1.786448e+07\n",
       "2018-05-12 11:01:31  0.000203      1.664305e+07\n",
       "2018-05-12 12:01:31  0.000395      3.387666e+07\n",
       "2018-05-12 13:01:31  0.000471      3.200991e+07"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the data\n",
    "test = pd.read_csv('../data/test_diff.csv', index_col=0)\n",
    "valid = pd.read_csv('../data/valid_diff.csv', index_col=0)\n",
    "train = pd.read_csv('../data/train_diff.csv', index_col=0)\n",
    "#set index to datetime periods\n",
    "#test.index = pd.to_datetime(test.index).to_period('H')\n",
    "#valid.index = pd.to_datetime(valid.index).to_period('H')\n",
    "#train.index = pd.to_datetime(train.index).to_period('H')\n",
    "#set index to datetime\n",
    "test.index = pd.to_datetime(test.index)\n",
    "valid.index = pd.to_datetime(valid.index)\n",
    "train.index = pd.to_datetime(train.index)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "isolated-priority",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p2</th>\n",
       "      <th>int_deliv_inv_ub</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.655000e+03</td>\n",
       "      <td>2.655000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.244946e-17</td>\n",
       "      <td>-1.901597e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000188e+00</td>\n",
       "      <td>1.000188e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.961264e+00</td>\n",
       "      <td>-8.967521e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-2.381792e-01</td>\n",
       "      <td>-8.967502e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.085614e-01</td>\n",
       "      <td>-4.075998e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.130729e-01</td>\n",
       "      <td>8.383006e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.166144e+00</td>\n",
       "      <td>2.449851e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 p2  int_deliv_inv_ub\n",
       "count  2.655000e+03      2.655000e+03\n",
       "mean   3.244946e-17     -1.901597e-16\n",
       "std    1.000188e+00      1.000188e+00\n",
       "min   -3.961264e+00     -8.967521e-01\n",
       "25%   -2.381792e-01     -8.967502e-01\n",
       "50%    2.085614e-01     -4.075998e-01\n",
       "75%    6.130729e-01      8.383006e-01\n",
       "max    2.166144e+00      2.449851e+00"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now we will scale the data\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "scaler = StandardScaler()\n",
    "y_scaler = StandardScaler() #we'll use the y-scaler later\n",
    "y_scaler.fit(train[['p2']])\n",
    "train[['p2', 'int_deliv_inv_ub']] = scaler.fit_transform(train)\n",
    "train.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "according-field",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p2</th>\n",
       "      <th>int_deliv_inv_ub</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1079.000000</td>\n",
       "      <td>1079.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.019547</td>\n",
       "      <td>-0.047890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.983953</td>\n",
       "      <td>1.041270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-4.176854</td>\n",
       "      <td>-0.896752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.104290</td>\n",
       "      <td>-0.896752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.124928</td>\n",
       "      <td>-0.806050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.553043</td>\n",
       "      <td>0.889583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.153585</td>\n",
       "      <td>2.358973</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                p2  int_deliv_inv_ub\n",
       "count  1079.000000       1079.000000\n",
       "mean      0.019547         -0.047890\n",
       "std       0.983953          1.041270\n",
       "min      -4.176854         -0.896752\n",
       "25%      -0.104290         -0.896752\n",
       "50%       0.124928         -0.806050\n",
       "75%       0.553043          0.889583\n",
       "max       3.153585          2.358973"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid[['p2', 'int_deliv_inv_ub']] = scaler.transform(valid)\n",
    "valid.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "according-faculty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p2</th>\n",
       "      <th>int_deliv_inv_ub</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1166.000000</td>\n",
       "      <td>1166.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.035744</td>\n",
       "      <td>-0.625483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.604301</td>\n",
       "      <td>0.685445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.867696</td>\n",
       "      <td>-0.896752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.083353</td>\n",
       "      <td>-0.896752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.066339</td>\n",
       "      <td>-0.896752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.261708</td>\n",
       "      <td>-0.896750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.375856</td>\n",
       "      <td>2.184141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                p2  int_deliv_inv_ub\n",
       "count  1166.000000       1166.000000\n",
       "mean      0.035744         -0.625483\n",
       "std       0.604301          0.685445\n",
       "min      -3.867696         -0.896752\n",
       "25%      -0.083353         -0.896752\n",
       "50%       0.066339         -0.896752\n",
       "75%       0.261708         -0.896750\n",
       "max       2.375856          2.184141"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[['p2', 'int_deliv_inv_ub']] = scaler.transform(test)\n",
    "test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emotional-visit",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "metallic-bidder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.59420064  0.25958877]\n",
      " [ 0.44594011  0.18332401]\n",
      " [ 0.45604353  0.10947734]\n",
      " [ 0.87168012  1.15141108]\n",
      " [ 1.03762608  1.0385484 ]\n",
      " [ 0.68422353  0.92382076]\n",
      " [ 0.38075093  0.62462487]\n",
      " [ 0.89124915  0.35741122]\n",
      " [ 0.32194582  0.3213169 ]\n",
      " [-1.18816483  0.66245371]]\n",
      "[-1.67139609]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2645, 10, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of lag variables and horizon (these are just some starting choices)\n",
    "T = 10 #the past N hours to study\n",
    "HORIZON = 1 #predict the next N hours\n",
    "\n",
    "#sample tensor structure\n",
    "tensor_structure = {'X':(range(-T+1, 1), ['p2', 'int_deliv_inv_ub'])}\n",
    "\n",
    "#training tensor\n",
    "train_input = TimeSeriesTensor(\n",
    "    dataset=train,\n",
    "    target=\"p2\",\n",
    "    H=HORIZON,\n",
    "    tensor_structure=tensor_structure,\n",
    "    freq=\"H\", #hours\n",
    "    drop_incomplete=True, \n",
    ")\n",
    "\n",
    "#validation tensor\n",
    "valid_inputs = TimeSeriesTensor(valid, \"p2\", HORIZON, tensor_structure)\n",
    "print(train_input['X'][0])\n",
    "print(train_input['target'][0])\n",
    "train_input['target'].shape\n",
    "train_input['X'].shape\n",
    "\n",
    "#print(valid_inputs['X'][0])\n",
    "#print(valid_inputs['target'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "coupled-empty",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we bring in the keras tensorflow model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import GRU, Dense\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.losses import MeanSquaredLogarithmicError, MeanSquaredError\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "#from keras_visualizer import visualizer\n",
    "\n",
    "from math import pow, floor\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "medical-india",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate schedule\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.0001\n",
    "    drop = 0.5\n",
    "    epochs_drop = 200\n",
    "    lrate = initial_lrate * pow(drop, floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "\n",
    "def my_loss_fn(y_true, y_pred):\n",
    "    return y_true - y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "figured-reading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v. 0 Latent_dim:  50 L2 Regularizer:  0.0 Optimizer: Adam Batch size: 100\n",
      "Epoch 1/1000\n",
      "27/27 [==============================] - 4s 75ms/step - loss: 1.0856 - val_loss: 0.9609\n",
      "Epoch 2/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.9692 - val_loss: 0.8918\n",
      "Epoch 3/1000\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.8701 - val_loss: 0.8317\n",
      "Epoch 4/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.8189 - val_loss: 0.7778\n",
      "Epoch 5/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.7518 - val_loss: 0.7308\n",
      "Epoch 6/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.7237 - val_loss: 0.6887\n",
      "Epoch 7/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.6815 - val_loss: 0.6523\n",
      "Epoch 8/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.6134 - val_loss: 0.6202\n",
      "Epoch 9/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.6100 - val_loss: 0.5902\n",
      "Epoch 10/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.5485 - val_loss: 0.5637\n",
      "Epoch 11/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.5275 - val_loss: 0.5395\n",
      "Epoch 12/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.5188 - val_loss: 0.5163\n",
      "Epoch 13/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.4743 - val_loss: 0.4947\n",
      "Epoch 14/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.4667 - val_loss: 0.4742\n",
      "Epoch 15/1000\n",
      "27/27 [==============================] - 0s 6ms/step - loss: 0.4357 - val_loss: 0.4553\n",
      "Epoch 16/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.4180 - val_loss: 0.4376\n",
      "Epoch 17/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.4221 - val_loss: 0.4198\n",
      "Epoch 18/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.3998 - val_loss: 0.4039\n",
      "Epoch 19/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.3981 - val_loss: 0.3887\n",
      "Epoch 20/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.3665 - val_loss: 0.3754\n",
      "Epoch 21/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.3496 - val_loss: 0.3623\n",
      "Epoch 22/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.3513 - val_loss: 0.3497\n",
      "Epoch 23/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.3419 - val_loss: 0.3383\n",
      "Epoch 24/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.3192 - val_loss: 0.3276\n",
      "Epoch 25/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.3275 - val_loss: 0.3168\n",
      "Epoch 26/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.3373 - val_loss: 0.3074\n",
      "Epoch 27/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.2979 - val_loss: 0.2993\n",
      "Epoch 28/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.3035 - val_loss: 0.2908\n",
      "Epoch 29/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.2860 - val_loss: 0.2828\n",
      "Epoch 30/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.2749 - val_loss: 0.2764\n",
      "Epoch 31/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.2576 - val_loss: 0.2699\n",
      "Epoch 32/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.2707 - val_loss: 0.2634\n",
      "Epoch 33/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.2709 - val_loss: 0.2576\n",
      "Epoch 34/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.2632 - val_loss: 0.2520\n",
      "Epoch 35/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.2441 - val_loss: 0.2475\n",
      "Epoch 36/1000\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.2438 - val_loss: 0.2424\n",
      "Epoch 37/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.2480 - val_loss: 0.2383\n",
      "Epoch 38/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.2464 - val_loss: 0.2341\n",
      "Epoch 39/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.2329 - val_loss: 0.2300\n",
      "Epoch 40/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.2434 - val_loss: 0.2265\n",
      "Epoch 41/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.2231 - val_loss: 0.2230\n",
      "Epoch 42/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.2346 - val_loss: 0.2199\n",
      "Epoch 43/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.2174 - val_loss: 0.2165\n",
      "Epoch 44/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.2163 - val_loss: 0.2135\n",
      "Epoch 45/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.2107 - val_loss: 0.2107\n",
      "Epoch 46/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.2234 - val_loss: 0.2079\n",
      "Epoch 47/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.2257 - val_loss: 0.2057\n",
      "Epoch 48/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.2136 - val_loss: 0.2032\n",
      "Epoch 49/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.2217 - val_loss: 0.2010\n",
      "Epoch 50/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.2079 - val_loss: 0.1987\n",
      "Epoch 51/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.2176 - val_loss: 0.1969\n",
      "Epoch 52/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.2087 - val_loss: 0.1949\n",
      "Epoch 53/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.2025 - val_loss: 0.1928\n",
      "Epoch 54/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.2083 - val_loss: 0.1911\n",
      "Epoch 55/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1982 - val_loss: 0.1895\n",
      "Epoch 56/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.2036 - val_loss: 0.1877\n",
      "Epoch 57/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1989 - val_loss: 0.1864\n",
      "Epoch 58/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.2047 - val_loss: 0.1849\n",
      "Epoch 59/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.2101 - val_loss: 0.1830\n",
      "Epoch 60/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1930 - val_loss: 0.1819\n",
      "Epoch 61/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1907 - val_loss: 0.1810\n",
      "Epoch 62/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1902 - val_loss: 0.1794\n",
      "Epoch 63/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1893 - val_loss: 0.1783\n",
      "Epoch 64/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1796 - val_loss: 0.1770\n",
      "Epoch 65/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1822 - val_loss: 0.1760\n",
      "Epoch 66/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1829 - val_loss: 0.1749\n",
      "Epoch 67/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1871 - val_loss: 0.1738\n",
      "Epoch 68/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1733 - val_loss: 0.1726\n",
      "Epoch 69/1000\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.1817 - val_loss: 0.1715\n",
      "Epoch 70/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1770 - val_loss: 0.1708\n",
      "Epoch 71/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1835 - val_loss: 0.1698\n",
      "Epoch 72/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1719 - val_loss: 0.1688\n",
      "Epoch 73/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1913 - val_loss: 0.1679\n",
      "Epoch 74/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1823 - val_loss: 0.1668\n",
      "Epoch 75/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1693 - val_loss: 0.1660\n",
      "Epoch 76/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1557 - val_loss: 0.1654\n",
      "Epoch 77/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1720 - val_loss: 0.1645\n",
      "Epoch 78/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1767 - val_loss: 0.1638\n",
      "Epoch 79/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1775 - val_loss: 0.1629\n",
      "Epoch 80/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1616 - val_loss: 0.1621\n",
      "Epoch 81/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1804 - val_loss: 0.1612\n",
      "Epoch 82/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1567 - val_loss: 0.1606\n",
      "Epoch 83/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1574 - val_loss: 0.1598\n",
      "Epoch 84/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1587 - val_loss: 0.1590\n",
      "Epoch 85/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1676 - val_loss: 0.1581\n",
      "Epoch 86/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1604 - val_loss: 0.1579\n",
      "Epoch 87/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1696 - val_loss: 0.1569\n",
      "Epoch 88/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1624 - val_loss: 0.1564\n",
      "Epoch 89/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1598 - val_loss: 0.1559\n",
      "Epoch 90/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1530 - val_loss: 0.1549\n",
      "Epoch 91/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1612 - val_loss: 0.1545\n",
      "Epoch 92/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1622 - val_loss: 0.1538\n",
      "Epoch 93/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1511 - val_loss: 0.1533\n",
      "Epoch 94/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1552 - val_loss: 0.1525\n",
      "Epoch 95/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1649 - val_loss: 0.1521\n",
      "Epoch 96/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1484 - val_loss: 0.1514\n",
      "Epoch 97/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1670 - val_loss: 0.1507\n",
      "Epoch 98/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1543 - val_loss: 0.1502\n",
      "Epoch 99/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1444 - val_loss: 0.1498\n",
      "Epoch 100/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1525 - val_loss: 0.1492\n",
      "Epoch 101/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1660 - val_loss: 0.1485\n",
      "Epoch 102/1000\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.1504 - val_loss: 0.1481\n",
      "Epoch 103/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1529 - val_loss: 0.1475\n",
      "Epoch 104/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1639 - val_loss: 0.1469\n",
      "Epoch 105/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1619 - val_loss: 0.1466\n",
      "Epoch 106/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1455 - val_loss: 0.1460\n",
      "Epoch 107/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1552 - val_loss: 0.1457\n",
      "Epoch 108/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1588 - val_loss: 0.1453\n",
      "Epoch 109/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1397 - val_loss: 0.1447\n",
      "Epoch 110/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1481 - val_loss: 0.1444\n",
      "Epoch 111/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1351 - val_loss: 0.1440\n",
      "Epoch 112/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1508 - val_loss: 0.1432\n",
      "Epoch 113/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1387 - val_loss: 0.1431\n",
      "Epoch 114/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1507 - val_loss: 0.1426\n",
      "Epoch 115/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1475 - val_loss: 0.1424\n",
      "Epoch 116/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1425 - val_loss: 0.1421\n",
      "Epoch 117/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1468 - val_loss: 0.1416\n",
      "Epoch 118/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1415 - val_loss: 0.1413\n",
      "Epoch 119/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1386 - val_loss: 0.1409\n",
      "Epoch 120/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1396 - val_loss: 0.1405\n",
      "Epoch 121/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1446 - val_loss: 0.1403\n",
      "Epoch 122/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1295 - val_loss: 0.1399\n",
      "Epoch 123/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1353 - val_loss: 0.1396\n",
      "Epoch 124/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1272 - val_loss: 0.1394\n",
      "Epoch 125/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1469 - val_loss: 0.1391\n",
      "Epoch 126/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1298 - val_loss: 0.1391\n",
      "Epoch 127/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1375 - val_loss: 0.1385\n",
      "Epoch 128/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1319 - val_loss: 0.1382\n",
      "Epoch 129/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1320 - val_loss: 0.1381\n",
      "Epoch 130/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1382 - val_loss: 0.1379\n",
      "Epoch 131/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1318 - val_loss: 0.1377\n",
      "Epoch 132/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1351 - val_loss: 0.1374\n",
      "Epoch 133/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1392 - val_loss: 0.1375\n",
      "Epoch 134/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1405 - val_loss: 0.1371\n",
      "Epoch 135/1000\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.1456 - val_loss: 0.1370\n",
      "Epoch 136/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1399 - val_loss: 0.1364\n",
      "Epoch 137/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1367 - val_loss: 0.1363\n",
      "Epoch 138/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1353 - val_loss: 0.1361\n",
      "Epoch 139/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1279 - val_loss: 0.1359\n",
      "Epoch 140/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1345 - val_loss: 0.1360\n",
      "Epoch 141/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1353 - val_loss: 0.1359\n",
      "Epoch 142/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1337 - val_loss: 0.1355\n",
      "Epoch 143/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1453 - val_loss: 0.1355\n",
      "Epoch 144/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1449 - val_loss: 0.1354\n",
      "Epoch 145/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1399 - val_loss: 0.1354\n",
      "Epoch 146/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1410 - val_loss: 0.1349\n",
      "Epoch 147/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1377 - val_loss: 0.1351\n",
      "Epoch 148/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1408 - val_loss: 0.1349\n",
      "Epoch 149/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1403 - val_loss: 0.1347\n",
      "Epoch 150/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1477 - val_loss: 0.1349\n",
      "Epoch 151/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1289 - val_loss: 0.1346\n",
      "Epoch 152/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1336 - val_loss: 0.1344\n",
      "Epoch 153/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1451 - val_loss: 0.1347\n",
      "Epoch 154/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1337 - val_loss: 0.1341\n",
      "Epoch 155/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1502 - val_loss: 0.1341\n",
      "Epoch 156/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1431 - val_loss: 0.1341\n",
      "Epoch 157/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1318 - val_loss: 0.1343\n",
      "Epoch 158/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1304 - val_loss: 0.1338\n",
      "Epoch 159/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1314 - val_loss: 0.1337\n",
      "Epoch 160/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1331 - val_loss: 0.1337\n",
      "Epoch 161/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1449 - val_loss: 0.1336\n",
      "Epoch 162/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1385 - val_loss: 0.1335\n",
      "Epoch 163/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1348 - val_loss: 0.1337\n",
      "Epoch 164/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1362 - val_loss: 0.1334\n",
      "Epoch 165/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1286 - val_loss: 0.1331\n",
      "Epoch 166/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1345 - val_loss: 0.1334\n",
      "Epoch 167/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1343 - val_loss: 0.1333\n",
      "Epoch 168/1000\n",
      "27/27 [==============================] - 0s 11ms/step - loss: 0.1358 - val_loss: 0.1333\n",
      "Epoch 169/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1381 - val_loss: 0.1330\n",
      "Epoch 170/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1264 - val_loss: 0.1331\n",
      "Epoch 171/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1445 - val_loss: 0.1328\n",
      "Epoch 172/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1336 - val_loss: 0.1330\n",
      "Epoch 173/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1371 - val_loss: 0.1330\n",
      "Epoch 174/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1418 - val_loss: 0.1329\n",
      "Epoch 175/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1320 - val_loss: 0.1327\n",
      "Epoch 176/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1321 - val_loss: 0.1328\n",
      "Epoch 177/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1355 - val_loss: 0.1327\n",
      "Epoch 178/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1435 - val_loss: 0.1328\n",
      "Epoch 179/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1355 - val_loss: 0.1325\n",
      "Epoch 180/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1288 - val_loss: 0.1325\n",
      "Epoch 181/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1383 - val_loss: 0.1323\n",
      "Epoch 182/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1273 - val_loss: 0.1325\n",
      "Epoch 183/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1286 - val_loss: 0.1323\n",
      "Epoch 184/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1388 - val_loss: 0.1323\n",
      "Epoch 185/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1344 - val_loss: 0.1322\n",
      "Epoch 186/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1234 - val_loss: 0.1325\n",
      "Epoch 187/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1388 - val_loss: 0.1320\n",
      "Epoch 188/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1256 - val_loss: 0.1320\n",
      "Epoch 189/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1198 - val_loss: 0.1322\n",
      "Epoch 190/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1261 - val_loss: 0.1320\n",
      "Epoch 191/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1337 - val_loss: 0.1318\n",
      "Epoch 192/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1289 - val_loss: 0.1320\n",
      "Epoch 193/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1243 - val_loss: 0.1318\n",
      "Epoch 194/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1356 - val_loss: 0.1317\n",
      "Epoch 195/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1342 - val_loss: 0.1319\n",
      "Epoch 196/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1321 - val_loss: 0.1317\n",
      "Epoch 197/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1348 - val_loss: 0.1318\n",
      "Epoch 198/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1343 - val_loss: 0.1317\n",
      "Epoch 199/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1326 - val_loss: 0.1315\n",
      "Epoch 200/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1273 - val_loss: 0.1314\n",
      "Epoch 201/1000\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.1276 - val_loss: 0.1313\n",
      "Epoch 202/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1311 - val_loss: 0.1316\n",
      "Epoch 203/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1266 - val_loss: 0.1315\n",
      "Epoch 204/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1255 - val_loss: 0.1316\n",
      "Epoch 205/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1322 - val_loss: 0.1315\n",
      "Epoch 206/1000\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1313 - val_loss: 0.1314\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru (GRU)                    (None, 50)                8100      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 8,151\n",
      "Trainable params: 8,151\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n",
      "\tFinal Val Loss: 0.13139531016349792  in: 41.99052953720093s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#best params:\n",
    "#Latent_dim:  50 L2 Regularizer:  0.0\n",
    "#Final Val Loss: 0.14\n",
    "#optimizer Adam\n",
    "#batch size 100\n",
    "#initial rate 0.0001\n",
    "#drop 0.5\n",
    "#epochs drop 50\n",
    "reg_vals = [0.0]\n",
    "optimizers = ['Adam']\n",
    "val_results = []\n",
    "batch_size = [100]\n",
    "ndims = [50]\n",
    "losses = [MeanSquaredError()]\n",
    "for loss in losses:\n",
    "    for ndim in ndims:\n",
    "        for reg_val in reg_vals:\n",
    "            for optimizer in optimizers:\n",
    "                for batch in batch_size:\n",
    "                    for i in range(0,1):\n",
    "                        print('v.',i,'Latent_dim: ',ndim,'L2 Regularizer: ', reg_val,'Optimizer:',optimizer, 'Batch size:',batch)\n",
    "\n",
    "\n",
    "                        t_start = time.time()\n",
    "                        LATENT_DIM = ndim\n",
    "                        BATCH_SIZE = batch\n",
    "                        EPOCHS = 1000\n",
    "\n",
    "                        lrate = LearningRateScheduler(step_decay)\n",
    "                        model = Sequential()\n",
    "                        model.add(GRU(LATENT_DIM, input_shape=(T, 2), kernel_regularizer=l2(reg_val), recurrent_regularizer=l2(reg_val), bias_regularizer=l2(reg_val)))\n",
    "                        #model.add(LSTM(32, kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
    "                        model.add(Dense(HORIZON))\n",
    "                        model.compile(optimizer=optimizer, loss=loss, \n",
    "                                     # metrics=['accuracy']\n",
    "                                     )\n",
    "\n",
    "                        earlystop = EarlyStopping(monitor=\"val_loss\", min_delta=0.0, patience=5)\n",
    "                        model_history = model.fit(\n",
    "                        train_input[\"X\"],\n",
    "                        train_input[\"target\"],\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        epochs=EPOCHS,\n",
    "                        validation_data=(valid_inputs[\"X\"], valid_inputs[\"target\"]),\n",
    "                        callbacks=[earlystop, lrate],\n",
    "                        verbose=1,\n",
    "                        )\n",
    "                        \n",
    "                        # list all data in history\n",
    "                        #print(model_history.history.keys())\n",
    "                        total_time = time.time()-t_start\n",
    "                        #make a model picture\n",
    "                        model.summary()\n",
    "                        #model_vis = visualizer(model, format='png', view=True)\n",
    "                        model_vis = plot_model(model, to_file='model.png')\n",
    "                        # summarize history for loss\n",
    "                        fig, ax = plt.subplots()\n",
    "                        ax.plot(model_history.history['loss'])\n",
    "                        ax.plot(model_history.history['val_loss'])\n",
    "\n",
    "                        ax.set_ylabel('loss')\n",
    "                        ax.set_xlabel('epoch')\n",
    "                        fig.legend(['loss', 'val_loss'], loc='upper left')\n",
    "                        #plt.show()\n",
    "                        print('\\tFinal Val Loss:',model_history.history['val_loss'][-1],' in: {}s'.format(total_time))\n",
    "                        if(model_history.history['val_loss'][-1] < 1.0):\n",
    "                            val_results.append((ndim,reg_val,model_history.history['val_loss'][-1],optimizer,total_time,batch,fig,model_vis))\n",
    "                        else:\n",
    "                            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "peaceful-franchise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 100\n",
      "\tAvg final val loss: 0.13+-nan\n",
      "\tAvg training time: 41.99+-nan\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'savefig'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-0cd6637165d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss-curve_batch-size_{}_{}.png'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'savefig'"
     ]
    }
   ],
   "source": [
    "#for result in val_results:\n",
    "  #  print(\"ndim: \",result[0])\n",
    "  #  print(\"reg_val: \",result[1])\n",
    "  #  print(\"val_loss: \",result[2])\n",
    "  #  print(\"optimizer: \",result[3])\n",
    "df_results = pd.DataFrame(val_results, columns=['hidden_dimensions','L2_reg_val','final_val_loss','optimizer','training_time','batch_size','FIG','MODEL_VIS'])\n",
    "res_grps = df_results.groupby('batch_size')\n",
    "for batch in batch_size:\n",
    "    print('Batch size: {}'.format(batch))\n",
    "    df_batch = res_grps.get_group(batch).drop(['batch_size','hidden_dimensions','L2_reg_val'],axis=1)\n",
    "    print('\\tAvg final val loss: {:.2f}+-{:.2f}'.format(df_batch.final_val_loss.mean(),df_batch.final_val_loss.std()))\n",
    "    print('\\tAvg training time: {:.2f}+-{:.2f}'.format(df_batch.training_time.mean(),df_batch.training_time.std()))\n",
    "    \n",
    "for count,result in enumerate(val_results):\n",
    "    result[-1].savefig('loss-curve_batch-size_{}_{}.png'.format(result[-4],count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-blank",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toxic-transformation",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model_history.history['loss'])\n",
    "plt.plot(model_history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('mean squared-error')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training loss', 'validation loss'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-pocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we'll test\n",
    "data_test_inputs = TimeSeriesTensor(test, \"p2\", HORIZON, tensor_structure)\n",
    "#print(data_test_inputs['X'][0])\n",
    "#print(data_test_inputs['target'][0])\n",
    "\n",
    "predictions = model.predict(data_test_inputs[\"X\"])\n",
    "#print(predictions)\n",
    "ev_data = create_evaluation_df(predictions, data_test_inputs, HORIZON, y_scaler)\n",
    "ev_groups = ev_data.groupby('h')\n",
    "#test.head()\n",
    "gr1 = ev_groups.get_group('t+1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-professional",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-meaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "apes = []\n",
    "for i in range(0, gr1.prediction.count()):\n",
    "    if gr1.actual.iloc[i] == 0:\n",
    "        continue\n",
    "    ape = abs((gr1.prediction.iloc[i] - gr1.actual.iloc[i]) / gr1.actual.iloc[i])\n",
    "    apes.append(ape)\n",
    "    \n",
    "mape1 = (sum(apes) / len(apes))\n",
    "\n",
    "print('Evaluating GRU')\n",
    "print(\"MAPE prediction 1 hour  ahead: {:.2%}\".format(mape1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaning-morrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets plot our results\n",
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "\n",
    "ev_group = ev_groups.get_group('t+1')\n",
    "\n",
    "ax.set_title('{time:.0f} hours predicting next 1 hour'.format(time=T))\n",
    "ax.set_ylabel('calibration change')\n",
    "ax.set_xlabel('FED firing time')\n",
    "plt.setp(ax.get_xticklabels(), rotation=30, horizontalalignment='right')\n",
    "\n",
    "ax.plot(ev_group.timestamp, ev_group.actual, \n",
    "         label='actual calibration', color='blue', linewidth=1, linestyle='solid')\n",
    "\n",
    "ax.plot(ev_group.timestamp, ev_group.prediction, \n",
    "         label='predicted calibration', color='red', linewidth=1, linestyle='solid')\n",
    "#legend\n",
    "fig.legend(ncol=1, loc = (0.8,0.2))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arranged-belly",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laughing-baker",
   "metadata": {},
   "outputs": [],
   "source": [
    "####MAKE DIFF PLOT########\n",
    "apdiff_series = ((ev_group.actual - ev_group.prediction) / ev_group.actual).abs()\n",
    "fig3, ax3 = plt.subplots(figsize=(15,5))\n",
    "\n",
    "ax3.set_ylabel('| Actual - Prediction / Actual |')\n",
    "ax3.set_xlabel('FED firing time')\n",
    "plt.setp(ax3.get_xticklabels(), rotation=30, horizontalalignment='right')\n",
    "\n",
    "#actual data\n",
    "ax3.plot(ev_group.timestamp, apdiff_series, \n",
    "     label='True value', color='green', linewidth=1, linestyle='solid')\n",
    "#legend\n",
    "fig3.legend(ncol=3, loc = 'upper center')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-insulin",
   "metadata": {},
   "outputs": [],
   "source": [
    "#our data is transformed to feed into the model. The diff step transforms the data by removing the large scale trends. We'll run that in reverse to look at the data with the trends.\n",
    "ev_data_int = ev_data.copy()\n",
    "ev_data_int['prediction'] = ev_data_int['prediction'].cumsum()\n",
    "ev_data_int['actual'] = ev_data_int['actual'].cumsum()\n",
    "ev_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thirty-vacuum",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets plot our results\n",
    "fig10, ax13 = plt.subplots(figsize=(15,5))\n",
    "\n",
    "ax13.set_title('{time:.0f} hours predicting next hour'.format(time=T))\n",
    "ax13.set_ylabel('calibration value minus initial')\n",
    "ax13.set_xlabel('calibration time')\n",
    "plt.setp(ax13.get_xticklabels(), rotation=30, horizontalalignment='right')\n",
    "\n",
    "ax13.plot(ev_data_int.timestamp, ev_data_int.actual, \n",
    "         label='actual calibration', color='blue', linewidth=2, linestyle='solid')\n",
    "\n",
    "ax13.plot(ev_data_int.timestamp, ev_data_int.prediction, \n",
    "         label='predicted calibration', color='red', linewidth=2, linestyle='solid')\n",
    "\n",
    "#lumi data on alternate y axis\n",
    "#ax14 = ax13.twinx()\n",
    "#ax14.set_ylabel('integrated luminosity (/ub)')\n",
    "#\n",
    "#ax14.plot(test_lumi_avg.index, test_lumi_avg, \n",
    "#         label='lumi averaged', color='limegreen', linewidth=1, linestyle='dashed')\n",
    "\n",
    "#legend\n",
    "fig10.legend(ncol=1, loc=(0.8,0.2))\n",
    "#plt.ylim([-.05, ev_data_int.actual.max()*1.1])\n",
    "fig10.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-package",
   "metadata": {},
   "outputs": [],
   "source": [
    "apes = []\n",
    "for i in range(0, gr1.prediction.count()):\n",
    "    if gr1.actual.iloc[i] == 0:\n",
    "        continue\n",
    "    ape = abs((gr1.prediction.iloc[i] - gr1.actual.iloc[i]) / gr1.actual.iloc[i])\n",
    "    apes.append(ape)\n",
    "    \n",
    "mape1 = (sum(apes) / len(apes))\n",
    "mape1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-departure",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-moore",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
